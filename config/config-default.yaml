##################################################################
#                          General                               #
##################################################################

general:
    # List of characters to include in our dataset (the phrase CAN NOT use characters that are not defined here).
    # Separate each character with a comma with no ending comma present
    characters: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z

    # Valid modes are train, test, continue, both, and predict. test, continue, and predict requires a valid uid.
    mode: both

    # Either attention or context to trigger special behavior. Leave blank to run normally. Must be used together
    # with mode: predict to function
    special:

    # Turn debug mode on/off
    debug: false


##################################################################
#                          Logging                               #
##################################################################

logging:
    batch_reporting: 10


##################################################################
#                          Various                               #
##################################################################

various:
    # Indicate if we should start tensorboard
    tensorboard: false

    # Save model only if save indicator is present. This is useful early in training as the
    # system will use a LOT time saving models every single epoch. Running the TF based attention
    # model, training and validating on a dataset of size 10k and 1k takes about 5 seconds on the GPU,
    # while storing the model takes about 30 seconds. Turning this indicator on does not actually save
    # the weights unless you have a file named 'save' in your output directory under the current uid.
    # Note: This functioality is only implemented for the TF callbacks.
    save-indicator: false


##################################################################
#                           Path                                 #
##################################################################

path:
    # Where to save all the images
    image: PROJECT_ROOT_SEP_data_SEP_image

    # Where to save the data (pickle files and session dumps)
    data: PROJECT_ROOT_SEP_data_SEP_data

    # Where to save the output (logs and images)
    output: PROJECT_ROOT_SEP_data_SEP_output


##################################################################
#                         Word list                              #
##################################################################

wordlist:
    # Set to true to remove duplicates within the same set
    remove-duplicate-set: true

    # Set to true to remove duplicates across all sets
    remove-duplicate-all: false


##################################################################
#                       Transformation                           #
##################################################################

transformation:
    # If noise handler is called, use this degree of randomness (in percent)
    noise-random-factor: 0

    # Seed to use for randomness
    noise-random-seed:

##################################################################
#                       Preprocessing                            #
##################################################################

preprocessing:
    run: true

    # Remove all earlier data on each run
    wipe: true

    # Randomize upper and lowercase (only use if both uppercase and lowercase characters are in the character list)
    random-upper-lower: false

    # Saves
    save:
        # The entire canvas (as defined in this config file)
        canvas: true

        # The cropped images
        cropped: true

        # The signatures
        signatures: true

    # Settings for text
    text:
        fonts:
        - arial-mono
        size: 35

    # Settings for the canvas we write on (should be irrelevant)
    canvas:
        width: 500
        height: 50

    # The signature stuff
    signature:
        height: 1
        position: 14

    # Various settings for the input
    input:
        # Maximum length of a word to classify
        max-length: 7

    # Training set
    training-set:
        size: 500

    # Validate set
    validate-set:
        size: 50

    # Test set
    test-set:
        size: 500


##################################################################
#                         Predicting                             #
##################################################################

predicting:
    run: true

    # How many epochs to run
    epochs: 20

    # Size of each batch. NOTE: This value must divide the size of training, validation and test sets
    batch-size: 50

    # The learning rate for our network
    learning-rate: 0.001

    # Specify the dimension of the embedding layer(s)
    embedding-dim: 128

    # Size of LSTM groups
    rnn-group-depth: 3

    # Class to lazy load for the prediction
    predictor: prediction.tensorflow.networks.sequence.EmbeddingAttentionSeq2SeqPredictor
    #predictor: prediction.tensorflow.networks.sequence.EmbeddingRNNSeq2SeqPredictor
    #predictor: prediction.keras.networks.lstm.LSTMEmbeddingVectorPredictor
